{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 2: K-Nearest Neighbours\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-01-18 | Josiah Wang | First version |\n",
        "2021-10-18 | Josiah Wang | Fixed typo in Distance Weighted K-NN Classifier evaluation. The last piece of code should say WeightedKNNClassifier(k) and not KNNClassifier(k), obviously! |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is to give you some practical experience to build a K-Nearest Neighbours (K-NN) classifier.\n",
        "\n",
        "By the end of this lab exercise, you will have constructed different variants of a K-NN classifier as discussed in the lectures."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k1PH_TJZhf-c"
      },
      "source": [
        "## The Iris dataset\n",
        "\n",
        "Continuing where we left off in Lab 1, we will again work with the Iris dataset.\n",
        "\n",
        "I will just copy and rerun my solutions from Lab 1. Feel free to copy your own implementations over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NdL1OHXvifcA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(120, 4)\n",
            "(30, 4)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "\n",
        "def read_dataset(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y, classes), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, K),\n",
        "                   where N is the number of instances\n",
        "                   K is the number of features/attributes\n",
        "               - y is a numpy array with shape (N, ), and should be integers from 0 to C-1\n",
        "                   where C is the number of classes\n",
        "               - classes : a numpy array with shape (C, ), which contains the\n",
        "                   unique class labels corresponding to the integers in y\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y_labels = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            x.append(list(map(float, row[:-1])))\n",
        "            y_labels.append(row[-1])\n",
        "\n",
        "    [classes, y] = np.unique(y_labels, return_inverse=True)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return (x, y, classes)\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_train, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n",
        "(x, y, classes) = read_dataset(\"iris.data\")\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2LNLsjjiwPJi"
      },
      "source": [
        "## K-NN Classifier\n",
        "\n",
        "In Lab 1, you have constructed a (one) Nearest Neighbour classifier.\n",
        "\n",
        "We will now try to generalise the nearest neighbour classifier as a **K-Nearest Neighours (K-NN)** classifier. For each test example, the classifier will predict the majority class label among the $K$ nearest training examples, again according to the Euclidean distance metric $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$. If there is draw, choose one of the majority class labels arbitrarily, or at random.\n",
        "\n",
        "Complete the `predict()` method of the `KNNClassifier` class. Note that the class now takes an optional hyperparameter `k` in its constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xtnx_HjTyA2N"
      },
      "outputs": [],
      "source": [
        "class KNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x_train = np.array([])\n",
        "        self.y_train = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x_train = x\n",
        "        self.y_train = y\n",
        "        \n",
        "    def predict(self, x_test):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO: Implement a K-NN classifier\n",
        "        results = []\n",
        "        for test_index, test_sample in enumerate(x_test):\n",
        "            distances = []\n",
        "            #for each test sample, we find the k closest neighbours\n",
        "            for train_index, train_sample in enumerate(self.x_train):\n",
        "                \n",
        "                sum = 0\n",
        "                for feature_index, feature in enumerate(train_sample):\n",
        "                    sum += (feature - test_sample[feature_index]) ** 2\n",
        "                dist = np.sqrt(sum)\n",
        "                #if dist == 0: print(\"aaa\")\n",
        "                #if dist == 0: print(\"Zero comparing test {a} at index {c} and training {b} at index {d}\".format(a=test_sample, b=train_sample, c=test_index, d=train_index))\n",
        "                distances.append([dist, train_index])\n",
        "\n",
        "            \n",
        "            #got the distances\n",
        "            distances = sorted(distances, key=lambda x: x[0])\n",
        "            # distances now contains the list [ [min, index], [2nd smallest, index], ... [largest, index] ]\n",
        "            # need to find the k smallest indexes and find the average of their labels\n",
        "\n",
        "            potential_labels = []\n",
        "            for i in range(self.k):\n",
        "                potential_labels.append(self.y_train[distances[i][1]])\n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "            sorted_labels = sorted(potential_labels)\n",
        "            #results.append(  sorted_labels[ int(np.floor(len(sorted_labels)/2)) ]  )\n",
        "            results.append( max(set(sorted_labels), key = sorted_labels.count) )\n",
        "            #print(\"of {a} labels {b} was chosen\".format(a=sorted_labels, b=max(set(sorted_labels), key = sorted_labels.count)))\n",
        "            #return 0\n",
        "\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JBpJ8EjVy-bi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 1, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "nn_classifier = KNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(nn_predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yZIOgepd2L"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now, let's evaluate the accuracy of our K-NN classifier. We will test K from 1 to 20. Again, you should be able to achieve very high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zfjzLLwUqDC-"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(y_gold, y_prediction):\n",
        "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "    y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "    y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "    float : the accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    try:\n",
        "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
        "    except ZeroDivisionError:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aWRWsYurz1om"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K=1: 0.9666666666666667\n",
            "K=2: 0.9666666666666667\n",
            "K=3: 1.0\n",
            "K=4: 1.0\n",
            "K=5: 1.0\n",
            "K=6: 1.0\n",
            "K=7: 0.9666666666666667\n",
            "K=8: 1.0\n",
            "K=9: 0.9666666666666667\n",
            "K=10: 1.0\n",
            "K=11: 1.0\n",
            "K=12: 0.9666666666666667\n",
            "K=13: 0.9666666666666667\n",
            "K=14: 0.9666666666666667\n",
            "K=15: 0.9666666666666667\n",
            "K=16: 0.9666666666666667\n",
            "K=17: 0.9666666666666667\n",
            "K=18: 0.9666666666666667\n",
            "K=19: 0.9666666666666667\n",
            "K=20: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = KNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9z8I2TXaLepZ"
      },
      "source": [
        "## Distance Weighted K-NN Classifier\n",
        "\n",
        "Now, let us implement the **distance weighted** K-NN classifier as discussed in the lectures.\n",
        "Pleae complete the `predict()` method for the `WeightedKNNClassifier` class below.\n",
        "\n",
        "For each test example $x^{(q)}$, you will need to compute the weight $w^{(i)}_{q}$ for each training example $x^{(i)}$.\n",
        "\n",
        "Then for each class label, sum the weights of the $K$ nearest examples. Assign the test example to the class label with largest sum.\n",
        "\n",
        "The weight can be anything reasonable. For this tutorial, we will use the weight $w_{q}^{(i)}=\\frac{1}{d(x^{(i)}, x^{(q)})}$, where $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$ is the Euclidean distance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "KTQ-cgA8LTFB"
      },
      "outputs": [],
      "source": [
        "class WeightedKNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x_train = np.array([])\n",
        "        self.y_train = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x_train = x\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        # TODO: Implement a distance weighted K-NN classifier\n",
        "        results = []\n",
        "        for test_sample in x_test:\n",
        "            distances = []\n",
        "            #for each test sample, we find the k closest neighbours\n",
        "            for train_index, train_sample in enumerate(self.x_train):\n",
        "                \n",
        "                sum = 0\n",
        "                for feature_index, feature in enumerate(train_sample):\n",
        "                    sum += (feature - test_sample[feature_index]) ** 2 #sum += (feature - test_sample[feature_index]) ** 2\n",
        "               \n",
        "                dist = np.sqrt(sum)\n",
        "                #if dist == 0: print(\"aaaa\")\n",
        "                weight = 1000000 if dist == 0 else 1/dist #there is repeated data in the iris.data file, so deep them identical\n",
        "                distances.append([dist, train_index, weight])\n",
        "\n",
        "            \n",
        "            #got the distances\n",
        "            distances = sorted(distances, key=lambda x: x[0])\n",
        "            #print(distances)\n",
        "\n",
        "            \n",
        "            included_samples = []\n",
        "            for i in range(self.k):\n",
        "                included_samples.append(distances[i])\n",
        "                \n",
        "            \n",
        "            outputs = {}\n",
        "            #print(included_samples)\n",
        "            #print(\"The length of distances is {a} and the length of the included samples is {b}\".format(a=len(distances), b=len(included_samples)))\n",
        "            for [distance, matchingIndex, weighting] in included_samples:\n",
        "                #print(self.y_train[matchingIndex])\n",
        "                if self.y_train[matchingIndex] in outputs.keys():\n",
        "                    outputs[self.y_train[matchingIndex]] += weighting\n",
        "                else:\n",
        "                    outputs[self.y_train[matchingIndex]] = weighting\n",
        "            \n",
        "           # print(outputs)\n",
        "            \n",
        "            \n",
        "            #print(\"weights: {}\".format(weights))\n",
        "\n",
        "            max_label = None\n",
        "            max_total = None\n",
        "            for key, value in outputs.items():\n",
        "                if max_label == None: max_label=key; max_total=value\n",
        "                elif value >= max_total:\n",
        "                    max_total=value\n",
        "                    max_label=key\n",
        "                \n",
        "            #print(max_label)\n",
        "            results.append(max_label)\n",
        "            #return        \n",
        "        return results\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Gjm_y7NnLTFP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 1, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "nn_classifier = WeightedKNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(nn_predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfaQ5WmgqSa"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Again, we will evaluate the accuracy of the distance weighted K-NN classifier for K=1 to 20. As this is a small and relatively easy dataset, you should get similar results as with the simple K-NN classifier.\n",
        "\n",
        "You might observe the benefits of the distance weighted K-NN classifier better when training on larger and noisier datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "bPr1YEfJ3g8Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K=1: 0.9666666666666667\n",
            "K=2: 0.9666666666666667\n",
            "K=3: 1.0\n",
            "K=4: 1.0\n",
            "K=5: 1.0\n",
            "K=6: 1.0\n",
            "K=7: 0.9666666666666667\n",
            "K=8: 1.0\n",
            "K=9: 0.9666666666666667\n",
            "K=10: 1.0\n",
            "K=11: 1.0\n",
            "K=12: 1.0\n",
            "K=13: 0.9666666666666667\n",
            "K=14: 0.9666666666666667\n",
            "K=15: 0.9666666666666667\n",
            "K=16: 0.9666666666666667\n",
            "K=17: 0.9666666666666667\n",
            "K=18: 0.9666666666666667\n",
            "K=19: 0.9666666666666667\n",
            "K=20: 0.9666666666666667\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = WeightedKNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You have built a K-Nearest Neighbour classifier (and the distance-weighted variant)! Because the Iris dataset is a small and simple dataset, you may only observe a small improvement in accuracy (if any) compared to a simple one-nearest neighbour classifier.\n",
        "\n",
        "And that is it for this week's lab tutorials. We will not have a decision tree tutorial for this course. The coursework itself will contain a short guide to help you think about implementing your own decision trees.\n",
        "\n",
        "In the next lab tutorial, you will be implementing different evaluation metrics and performing cross-validation.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
